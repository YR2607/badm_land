name: BWF Scrape Daily

on:
  schedule:
    - cron: '0 5 * * *'
  workflow_dispatch: {}

jobs:
  scrape:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      - name: Run scraper
        env:
          SCRAPERAPI_KEY: ${{ secrets.SCRAPERAPI_KEY }}
          SCRAPINGBEE_KEY: ${{ secrets.SCRAPINGBEE_KEY }}
          # Default: parse ONLY from championships main and news pages (diverse per-card images)
          BWF_CHAMP_ONLY: '1'
          BWF_CHAMP_URLS: 'https://bwfworldchampionships.bwfbadminton.com/,https://bwfworldchampionships.bwfbadminton.com/news/'
        run: |
          python scripts/bwf_scrape.py
      - name: Show scraped items count
        run: |
          python -c "import json; j=json.load(open('public/data/bwf_news.json','r',encoding='utf-8')); print('items:', len(j.get('items') or [])); print('first_titles:', [it.get('title') for it in (j.get('items') or [])][:3])"
      - name: Upload scraped JSON artifact
        uses: actions/upload-artifact@v4
        with:
          name: bwf_news_json
          path: public/data/bwf_news.json
      - name: Commit and push
        run: |
          git config user.name "github-actions"
          git config user.email "github-actions@users.noreply.github.com"
          git add public/data/bwf_news.json
          git commit -m "chore(scrape): update BWF news JSON - $(date '+%Y-%m-%d %H:%M')" || echo "No changes"
          git push
      - name: Note about Vercel auto-deploy
        run: |
          echo "Vercel will automatically deploy when we push to main branch"
          echo "No need for manual deploy hook to avoid double deployment"
