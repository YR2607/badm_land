name: BWF Scrape Daily

on:
  schedule:
    - cron: '0 5 * * *'
  workflow_dispatch: {}

jobs:
  scrape:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      - name: Run scraper
        env:
          SCRAPERAPI_KEY: ${{ secrets.SCRAPERAPI_KEY }}
          SCRAPINGBEE_KEY: ${{ secrets.SCRAPINGBEE_KEY }}
        run: |
          python scripts/bwf_scrape.py
      - name: Commit and push (skip if empty)
        run: |
          ITEMS_COUNT=$(python - <<'PY'
import json
import sys
try:
  with open('public/data/bwf_news.json','r',encoding='utf-8') as f:
    j=json.load(f)
  print(len(j.get('items',[]) or []))
except Exception:
  print(0)
PY
          )
          echo "items: $ITEMS_COUNT"
          if [ "$ITEMS_COUNT" -gt 0 ]; then
            git config user.name "github-actions"
            git config user.email "github-actions@users.noreply.github.com"
            git add public/data/bwf_news.json
            git commit -m "chore(scrape): update BWF news JSON" || echo "No changes"
            git push
            echo "Pushed updated JSON"
          else
            echo "Skip commit: items==0 (keeping previous JSON)"
          fi
      - name: Trigger Vercel deploy (optional)
        env:
          VERCEL_DEPLOY_HOOK_URL: ${{ secrets.VERCEL_DEPLOY_HOOK_URL }}
        run: |
          if [ -n "$VERCEL_DEPLOY_HOOK_URL" ]; then
            curl -X POST -sS "$VERCEL_DEPLOY_HOOK_URL" || true
          else
            echo "No VERCEL_DEPLOY_HOOK_URL secret set, skipping deploy hook"
          fi
