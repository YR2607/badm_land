name: BWF Scrape Daily

on:
  schedule:
    - cron: '0 5 * * *'
  workflow_dispatch: {}

jobs:
  scrape:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      - name: Run scraper
        env:
          SCRAPERAPI_KEY: ${{ secrets.SCRAPERAPI_KEY }}
          SCRAPINGBEE_KEY: ${{ secrets.SCRAPINGBEE_KEY }}
        run: |
          python scripts/bwf_scrape.py
      - name: Show scraped items count
        run: |
          python -c "import json; j=json.load(open('public/data/bwf_news.json','r',encoding='utf-8')); print('items:', len(j.get('items') or [])); print('first_titles:', [it.get('title') for it in (j.get('items') or [])][:3])"
      - name: Upload scraped JSON artifact
        uses: actions/upload-artifact@v4
        with:
          name: bwf_news_json
          path: public/data/bwf_news.json
      - name: Commit and push
        run: |
          git config user.name "github-actions"
          git config user.email "github-actions@users.noreply.github.com"
          git add public/data/bwf_news.json
          git commit -m "chore(scrape): update BWF news JSON" || echo "No changes"
          git push
      - name: Trigger Vercel deploy (optional)
        env:
          VERCEL_DEPLOY_HOOK_URL: ${{ secrets.VERCEL_DEPLOY_HOOK_URL }}
        run: |
          if [ -n "$VERCEL_DEPLOY_HOOK_URL" ]; then
            curl -X POST -sS "$VERCEL_DEPLOY_HOOK_URL" || true
          else
            echo "No VERCEL_DEPLOY_HOOK_URL secret set, skipping deploy hook"
          fi
