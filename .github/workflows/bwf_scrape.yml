name: BWF Scrape Daily

on:
  schedule:
    - cron: '0 5,11,17 * * *'  # Run 3 times daily: 5 AM, 11 AM, 5 PM UTC
  workflow_dispatch: {}
  push:
    branches: [ main ]
    paths:
      - 'scripts/bwf_scrape.py'
      - '.github/workflows/bwf_scrape.yml'

jobs:
  scrape:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      - name: Run scraper
        env:
          SCRAPERAPI_KEY: ${{ secrets.SCRAPERAPI_KEY }}
          SCRAPINGBEE_KEY: ${{ secrets.SCRAPINGBEE_KEY }}
          # Default: parse ONLY from championships main and news pages (diverse per-card images)
          BWF_CHAMP_ONLY: '1'
          BWF_CHAMP_URLS: 'https://bwfbadminton.com/news/,https://bwfbadminton.com/,https://bwfworldchampionships.bwfbadminton.com/news/,https://bwfworldchampionships.bwfbadminton.com/'
          # Force proxy for championship domain to avoid consent/cookie HTML on CI
          BWF_FORCE_PROXY: '1'
          # Use default mode to get full article data and latest news
          # BWF_MODE: 'list_only'
        run: |
          echo "Starting BWF scraper..."
          python scripts/bwf_scrape.py
          echo "Scraper completed with exit code: $?"
      - name: Show scraped items count
        run: |
          python -c "import json; j=json.load(open('public/data/bwf_news.json','r',encoding='utf-8')); print('items:', len(j.get('items') or [])); print('first_titles:', [it.get('title') for it in (j.get('items') or [])][:3])"
      - name: Diagnostics (unique images)
        run: |
          python - <<'PY'
          import json
          j=json.load(open('public/data/bwf_news.json','r',encoding='utf-8'))
          imgs=[(it.get('img') or '') for it in (j.get('items') or [])]
          uniq=list(dict.fromkeys(imgs))
          print('unique_images_count:', len([u for u in uniq if u]))
          print('sample_images:', uniq[:10])
          PY
      - name: Upload scraped JSON artifact
        uses: actions/upload-artifact@v4
        with:
          name: bwf_news_json
          path: public/data/bwf_news.json
      - name: Commit and push
        run: |
          git config user.name "github-actions"
          git config user.email "github-actions@users.noreply.github.com"
          git add public/data/bwf_news.json
          git commit -m "chore(scrape): update BWF news JSON - $(date '+%Y-%m-%d %H:%M')" || echo "No changes"
          git push
      - name: Note about Vercel auto-deploy
        run: |
          echo "Vercel will automatically deploy when we push to main branch"
          echo "No need for manual deploy hook to avoid double deployment"
